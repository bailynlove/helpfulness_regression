# !pip install textblob

# !python -m textblob.download_corpora
# import nltk

# nltk.download('punkt')
# nltk.download('wordnet')
# nltk.download('averaged_perceptron_tagger')
# /root/nltk_data
# !rm -r /root/nltk_data

# if missing some required data
#!git clone https://github.com/nltk/nltk_data.git
#!unzip ./nltk_data/packages/tokenizers/punkt_tab.zip
#!unzip ./nltk_data/packages/tokenizers/punkt.zip
# !rm -r /opt/anaconda3/envs/OnlineReview/nltk_data
# !cp -r ./nltk_data/packages /opt/anaconda3/envs/OnlineReview/nltk_data


import os

import numpy as np
import pandas as pd

from datetime import datetime
from textblob import TextBlob


from torch.utils.data import DataLoader, Dataset
import numpy as np
import torch.nn as nn
from torch.autograd import Variable
import torch.nn.functional as F
from torchvision import models, transforms
from collections import OrderedDict
from tqdm import tqdm
import pprint
import cv2
import random
import torch
import sys
%matplotlib inline
sys.path.append("..")





from model.resnet_FT import ResNetGAPFeatures as Net
from utils.data import read_data, create_dataloader, AestheticsDataset





use_cuda = torch.cuda.is_available()
print(use_cuda)
use_mps = torch.backends.mps.is_available()  # mac
print(use_mps)


# def device_set(tensor):
# resnet.to("mps")





# !cp ./resnet50-0676ba61.pth /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth
save_path = "./checkpoint/001" 
checkpoint = "epoch_19.loss_0.40747442577863185.pth"
resnet = models.resnet50(pretrained=True)
net = Net(resnet, n_features=12)
if use_cuda:
    resnet = resnet.cuda()
    net = net.cuda()
    net.load_state_dict(torch.load(f"{save_path}/{checkpoint}"))
elif use_mps:
    resnet = resnet.to("mps")
    net = net.to("mps")
    net.load_state_dict(torch.load(f"{save_path}/{checkpoint}", map_location='mps'))
else:
    net.load_state_dict(torch.load(f"{save_path}/{checkpoint}", map_location=lambda storage, loc: storage))


import time
t0 = time.time()
test = read_data("./image_data/review_data_test.csv", "../datasets/yelp_sfs_15g/res_images", is_test=True)
test_loader = create_dataloader(test, batch_size=1, is_train=False)
print(round(time.time() - t0, 2))





# attr_keys = ['BalacingElements', 'ColorHarmony', 'Content', 'DoF',
#              'Light', 'MotionBlur', 'Object', 'RuleOfThirds', 'VividColor']
# non_neg_attr_keys = ['Repetition', 'Symmetry', 'score']
# all_keys = attr_keys + non_neg_attr_keys
# used_keys = ["ColorHarmony", "Content", "DoF", "Object", "VividColor", "score"]


# def extract_prediction(inp, net):
#     d = dict()
#     net.eval()
#     output = net(inp)
#     # print(output)
#     for i, key in enumerate(all_keys):
#         # print(output[:, i])
#         # print(output[:, i].squeeze())
#         # d[key] = output[:, i].squeeze().data[0]
#         d[key] = output[:, i].squeeze().item()
#     return d


# def sample_data(dataset, image_path=None):
#     idx = random.sample(range(len(dataset)), 1)[0]
#     return dataset[idx]


test_dataset = AestheticsDataset(test, is_train=False)


len(test_dataset)


# data = test_dataset[0]
# image = data['image']
# image_path = data['image_path']
# image_default = mpimg.imread(image_path)
# img_shape = image_default.shape
# h, w = img_shape[0], img_shape[1]
# inp = Variable(image).unsqueeze(0)
# net.eval()
# output = net(inp)
# output


data


data = test_dataset[0]
image = data['image']
image_path = data['image_path']
image_default = mpimg.imread(image_path)
img_shape = image_default.shape
h, w = img_shape[0], img_shape[1]
# inp = Variable(image).unsqueeze(0)
inp = torch.tensor(image).unsqueeze(0)  # 直接转换为 Tensor，并增加 batch 维度
if use_cuda:
    inp = inp.cuda()
elif use_mps:
    inp = inp.to("mps")
# predicted_values = extract_prediction(inp, net)
score = net(inp)[:, -1].squeeze().item()
score


net.eval()  # 设置模型为评估模式（只需要一次）
for j in range(13):
    AQ_results_i = []
    for i in range(j * 10000, max(j * 10000 + 10000, len(test_dataset))):
        data = test_dataset[i]
        image = data['image']
        image_path = data['image_path']
        image_default = mpimg.imread(image_path)
        img_shape = image_default.shape
        h, w = img_shape[0], img_shape[1]
        # inp = Variable(image).unsqueeze(0)
        inp = torch.tensor(image).unsqueeze(0)  # 直接转换为 Tensor，并增加 batch 维度
        if use_cuda:
            inp = inp.cuda()
        elif use_mps:
            inp = inp.to("mps")
        # predicted_values = extract_prediction(inp, net)
        score = net(inp)[:, -1].squeeze().item()
        AQ_results_i.append({
            'review_id': data['review_id'],
            'image_file': data['image_file'],
            'score': score
        })
        if i % 1000 == 0:
            print(f"now data: {i}")
    print("I have to rest")
    time.sleep(10)
    pd.DataFrame(AQ_results_i).to_csv(f"./image_data/predict_{j}.csv")
    print(f"predict_{j} saved.")
    time.sleep(15)
        # aq_df = pd.concat(
        #     [aq_df, pd.DataFrame({'review_id': [data['review_id']], 'image': [data['image']], 'score': [score]})],
        #     axis=0, ignore_index=True)


# for i in range(120000, len(test_dataset)):
#     data = test_dataset[i]
#     image = data['image']
#     image_path = data['image_path']
#     image_default = mpimg.imread(image_path)
#     img_shape = image_default.shape
#     h, w = img_shape[0], img_shape[1]
#     # inp = Variable(image).unsqueeze(0)
#     inp = torch.tensor(image).unsqueeze(0)  # 直接转换为 Tensor，并增加 batch 维度
#     if use_cuda:
#         inp = inp.cuda()
#     elif use_mps:
#         inp = inp.to("mps")
#     # predicted_values = extract_prediction(inp, net)
#     score = net(inp)[:, -1].squeeze().item()
#     AQ_results_i.append({
#         'review_id': data['review_id'],
#         'image_file': data['image_file'],
#         'score': score
#     })
#     if i % 1000 == 0:
#         print(f"now data: {i}")
# print("I have to rest")
# time.sleep(10)
# pd.DataFrame(AQ_results_i).to_csv(f"./image_data/predict_12.csv")
# print(f"predict_12 saved.")





df = pd.read_csv('review_data.csv')
df





AQ_df = pd.DataFrame(columns=["review_id", "image_file", "score"])
for i in range(13):
    df_i = pd.read_csv(f"./image_data/predict_{i}.csv", header=0, index_col=0)
    # print(df_i)
    AQ_df = pd.concat([AQ_df, df_i], axis=0, ignore_index=True)
AQ_df


AQ_final = AQ_df.groupby(['review_id'])["score"].mean().reset_index()
AQ_final


v_df = pd.DataFrame()
v_df['review_id'] = df['review_id']
# Aesthetic Quality  sum from p (score_p) / No. of photos of review i
v_df = pd.merge(v_df, AQ_final, on='review_id', how='left')

v_df


# 计算 subjectivity 和 polarity
v_df['subjectivity'] = df['review'].apply(lambda x: TextBlob(x).sentiment.subjectivity)
v_df['polarity'] = df['review'].apply(lambda x: TextBlob(x).sentiment.polarity)

# 计算 AO 和 SE
v_df['AO'] = 1 - v_df['subjectivity']  # Argument Objectivity  1 - subjectivity_i  ranges from 0 to 1
v_df['SE'] = abs(v_df['polarity'] - 0.278)  # Sentiment Extremity   |sentiment_polarity_i - 0.278|  # sentiment polarity ranges from -1 to 1 where 0.278 is the average sentiment across all reviews
# Type_i = 1 if rating is 4 or 5 stars, else 0 (negative)
v_df['Type'] = np.where(df['rating'] >= 4, 1, 0)
# Perceived helpfulness. Number of “useful” votes review i received.
v_df['HV'] = df['helpfulness_votes']
# Number of photos that review i contains.
v_df['NoP'] = df['photo_ids'].str.split(',').str.len()  # v_df['NoP'] = df['photo_ids'].apply(lambda x: len(x.split(',')))
# Number of words in the text of review i
v_df['words'] = df['review'].apply(lambda x: len(TextBlob(x).words))
# Number of days elapsed since review i was posted.
v_df['days'] = (datetime.now() - pd.to_datetime(df['review_time'], format='%m/%d/%Y')).dt.days
# Number of reviewer u’s friends.
v_df['NoF'] = df['user_friend_count']
# Number of reviews the reviewer u posted.
v_df['NoR'] = df['user_review_count']
# Volume of consumer reviews of restaurant b.
v_df['volume_b'] = df['business_review_count']
# Overall consumer rating of restaurant b
v_df['rating'] = df['business_rating']
v_df





v_df_log = pd.DataFrame()
v_df_log["review_id"] = v_df["review_id"]
for col in v_df.columns:
    if col in ["review_id", "Type", "rating"]:
        v_df_log[col] = v_df[col]
    else:
        v_df_log[col] = np.log(v_df[col] + 1e-6)  # 对数变换：避免对数0，增加一个很小的常数
v_df_log


# !pip install scikit-learn
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_scaled = v_df_log['review_id']
df_scaled = pd.concat([df_scaled, pd.DataFrame(scaler.fit_transform(v_df_log[v_df_log.columns[1:]]), columns=v_df_log.columns[1:])], axis=1)

df_scaled





statsmodels


# !pip install statsmodels
import statsmodels.api as sm
from statsmodels.formula.api import mixedlm





model = mixedlm("HV ~ AQ + AO + SE + AQ * AO + AQ * SE + NoP", 
                df_scaled,
                groups=df_scaled["restaurant_id"],
                re_formula=None)  # 不加入任何随机效应
